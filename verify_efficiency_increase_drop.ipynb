{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/vzouhar/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "data = load_dataset(\"wikitext\", 'wikitext-103-v1', split=\"train\")\n",
    "data_1m = \"\"\n",
    "with open(\"data/wikitext_1m.txt\", \"w\") as f:\n",
    "    for line in data[\"text\"][:1_000_000]:\n",
    "        line = line.replace(\"<unk>\", \"\").strip()\n",
    "        if line:\n",
    "            f.write(line+\"\\n\")\n",
    "            data_1m += line+\"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 training\n",
      "\n",
      "\n",
      "\n",
      "8000 encoding\n",
      "28000 training\n",
      "\n",
      "\n",
      "\n",
      "28000 encoding\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import (\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "import collections\n",
    "dists = {}\n",
    "\n",
    "def get_bpe_unigram(vocab_size):\n",
    "    print(vocab_size, \"training\")\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size)\n",
    "    tokenizer.train([\"data/wikitext_1m.txt\"], trainer=trainer)\n",
    "    \n",
    "    print(vocab_size, \"encoding\")\n",
    "    encoding = tokenizer.encode(data_1m)\n",
    "    return collections.Counter(encoding.tokens)\n",
    "\n",
    "\n",
    "dists[\"bpe_8k\"] = get_bpe_unigram(8_000)\n",
    "dists[\"bpe_28k\"] = get_bpe_unigram(28_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64274162 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "gpt2tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "dists[\"gpt2\"] = collections.Counter(gpt2tokenizer.tokenize(data_1m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe_8k\n",
      "&  &  & 10.16 & 11.82 & 6.17 & 1.13 & 1.32 & 0.69\\\\\n",
      "& 2500 & 500 & 8.42 & 11.11 & 5.11 & 0.94 & 1.24 & 0.57\\\\\n",
      "& 2500 & 1000 & 7.72 & 10.66 & 5.10 & 0.87 & 1.20 & 0.58\\\\\n",
      "& $\\infty$ & 500 & 9.51 & 11.49 & 6.20 & 1.06 & 1.29 & 0.69\\\\\n",
      "& $\\infty$ & 1000 & 8.90 & 11.19 & 5.80 & 1.00 & 1.26 & 0.65\\\\\n",
      "\\hdashline \n",
      "\n",
      "bpe_28k\n",
      "&  &  & 10.65 & 13.29 & 5.83 & 1.04 & 1.30 & 0.57\\\\\n",
      "& 2500 & 500 & 9.16 & 12.74 & 5.53 & 0.90 & 1.25 & 0.54\\\\\n",
      "& 2500 & 1000 & 7.75 & 12.23 & 4.66 & 0.76 & 1.20 & 0.46\\\\\n",
      "& $\\infty$ & 500 & 10.50 & 13.19 & 5.94 & 1.03 & 1.29 & 0.58\\\\\n",
      "& $\\infty$ & 1000 & 10.34 & 13.10 & 5.98 & 1.01 & 1.28 & 0.59\\\\\n",
      "\\hdashline \n",
      "\n",
      "gpt2\n",
      "&  &  & 10.76 & 13.66 & 5.93 & 1.00 & 1.27 & 0.55\\\\\n",
      "& 2500 & 500 & 9.55 & 13.22 & 5.90 & 0.89 & 1.23 & 0.55\\\\\n",
      "& 2500 & 1000 & 8.27 & 12.77 & 5.17 & 0.77 & 1.19 & 0.48\\\\\n",
      "& $\\infty$ & 500 & 10.70 & 13.61 & 5.99 & 1.00 & 1.27 & 0.56\\\\\n",
      "& $\\infty$ & 1000 & 10.63 & 13.56 & 6.00 & 0.99 & 1.26 & 0.56\\\\\n",
      "\\hdashline \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def renyi_entropy(P, alpha):\n",
    "    scale = 1 / (1 - alpha)\n",
    "\n",
    "    return scale * np.log2(np.sum([\n",
    "        prob**alpha\n",
    "        for prob in P\n",
    "    ]))\n",
    "\n",
    "def renyi_eff(P, alpha):\n",
    "    return renyi_entropy(P, alpha)/np.log(len(P))\n",
    "\n",
    "def shannon_entropy(P):\n",
    "    P = np.array(P)\n",
    "    return -np.sum(P * np.log2(P))\n",
    "\n",
    "def shannon_eff(P):\n",
    "    return shannon_entropy(P)/np.log(len(P))\n",
    "\n",
    "def table_line(P, extra=[]):\n",
    "    out = extra\n",
    "    out.append(f\"{shannon_entropy(P):.2f}\")\n",
    "    out.append(f\"{renyi_entropy(P, 0.5):.2f}\")\n",
    "    out.append(f\"{renyi_entropy(P, 3):.2f}\")\n",
    "    out.append(f\"{shannon_eff(P):.2f}\")\n",
    "    out.append(f\"{renyi_eff(P, 0.5):.2f}\")\n",
    "    out.append(f\"{renyi_eff(P, 3):.2f}\")\n",
    "    return \"& \" + \" & \".join(out) + r\"\\\\\"\n",
    "\n",
    "def freqs_to_p(freqs):\n",
    "    total = sum(freqs.values())\n",
    "    return [v/total for k, v in freqs.most_common()]\n",
    "\n",
    "\n",
    "def drop_bpe(freqs, N, k):\n",
    "    freqs = copy.deepcopy(freqs)\n",
    "\n",
    "    # get top N\n",
    "    freq_words = [k for k, v in freqs.most_common(N)]\n",
    "\n",
    "    # sample k words\n",
    "    dead_tokens = random.sample(freq_words, k=k)\n",
    "    for token in dead_tokens:\n",
    "        # remove existing token\n",
    "        token_freq = freqs.pop(token)\n",
    "\n",
    "        # add the old frequency to the individual characters\n",
    "        for c in token:\n",
    "            freqs[c] += token_freq\n",
    "\n",
    "    # drop this special GPT token if it's an individual character if it's there\n",
    "    if \"Ġ\" in freqs:\n",
    "        freqs.pop(\"Ġ\")\n",
    "    return freqs_to_p(freqs)\n",
    "\n",
    "\n",
    "def drop_bpe(freqs, N, k):\n",
    "    # get top N\n",
    "    freq_words = [k for k, v in freqs.most_common(N)]\n",
    "\n",
    "    # sample k words\n",
    "    dead_tokens = random.sample(freq_words, k=k)\n",
    "    for token in dead_tokens:\n",
    "        # remove existing token\n",
    "        token_freq = freqs.pop(token)\n",
    "\n",
    "        # add the old frequency to the individual characters\n",
    "        for c in token:\n",
    "            freqs[c] += token_freq\n",
    "\n",
    "    return freqs_to_p(freqs)\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "for tokenizer, freqs in dists.items():\n",
    "    print(tokenizer)\n",
    "    print(table_line(freqs_to_p(freqs), extra=[\"\", \"\"]))\n",
    "    print(table_line(drop_bpe(freqs, 2_500, 500), extra=[\"2500\", \"500\"]))\n",
    "    print(table_line(drop_bpe(freqs, 2_500, 1000), extra=[\"2500\", \"1000\"]))\n",
    "    print(table_line(drop_bpe(freqs, None, 500), extra=[r\"$\\infty$\", \"500\"]))\n",
    "    print(table_line(drop_bpe(freqs, None, 1000), extra=[r\"$\\infty$\", \"1000\"]))\n",
    "    print(r\"\\hdashline\", \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
