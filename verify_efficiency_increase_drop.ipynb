{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "data = load_dataset(\"wikitext\", 'wikitext-103-v1', split=\"train\")\n",
    "data_1m = \"\"\n",
    "with open(\"data/wikitext_1m.txt\", \"w\") as f:\n",
    "    for line in data[\"text\"][:1_000_000]:\n",
    "        line = line.replace(\"<unk>\", \"\").strip()\n",
    "        if line:\n",
    "            f.write(line+\"\\n\")\n",
    "            data_1m += line+\"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 training\n",
      "\n",
      "\n",
      "\n",
      "8000 encoding\n",
      "28000 training\n",
      "\n",
      "\n",
      "\n",
      "28000 encoding\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import (\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "import collections\n",
    "dists = {}\n",
    "\n",
    "def get_bpe_unigram(vocab_size):\n",
    "    print(vocab_size, \"training\")\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size)\n",
    "    tokenizer.train([\"data/wikitext_1m.txt\"], trainer=trainer)\n",
    "    print(vocab_size, \"encoding\")\n",
    "    encoding = tokenizer.encode(data_1m)\n",
    "    return collections.Counter(encoding.tokens)\n",
    "\n",
    "\n",
    "dists[\"bpe_8k\"] = get_bpe_unigram(8_000)\n",
    "dists[\"bpe_28k\"] = get_bpe_unigram(28_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "gpt2tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "dists[\"gpt2\"] = collections.Counter(gpt2tokenizer.tokenize(data_1m))\n",
    "new_collection = collections.Counter()\n",
    "for k,v in dists[\"gpt2\"].items():\n",
    "    new_collection[k.replace(\"Ä \", \"\")] = v\n",
    "dists[\"gpt2\"] = new_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_to_unmerges(freqs):\n",
    "    # TODO: this is observed vocab which might not be the real one\n",
    "    vocab = set(freqs.keys())\n",
    "    unmerges = {}\n",
    "    for word in vocab:\n",
    "        for i in range(len(word)):\n",
    "            if word[:i] in vocab and word[i:] in vocab:\n",
    "                unmerges[word] = (word[:i], word[i:])\n",
    "                # go to next word\n",
    "                break\n",
    "    return unmerges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe_8k\n",
      "&  &  & 10.16 & 11.82 & 6.17 & 1.13 & 1.32 & 0.69\\\\\n",
      "& 2500 & 500 & 9.46 & 11.47 & 5.70 & 1.06 & 1.28 & 0.64\\\\\n",
      "& 2500 & 1000 & 9.05 & 11.20 & 5.84 & 1.02 & 1.26 & 0.66\\\\\n",
      "& $\\infty$ & 500 & 9.96 & 11.65 & 6.26 & 1.12 & 1.31 & 0.70\\\\\n",
      "& $\\infty$ & 1000 & 9.62 & 11.44 & 6.25 & 1.09 & 1.29 & 0.71\\\\\n",
      "\\hdashline \n",
      "\n",
      "bpe_28k\n",
      "&  &  & 10.65 & 13.29 & 5.83 & 1.04 & 1.30 & 0.57\\\\\n",
      "& 2500 & 500 & 10.43 & 13.14 & 5.99 & 1.02 & 1.29 & 0.59\\\\\n",
      "& 2500 & 1000 & 9.31 & 12.77 & 5.21 & 0.91 & 1.25 & 0.51\\\\\n",
      "& $\\infty$ & 500 & 10.63 & 13.25 & 5.85 & 1.04 & 1.30 & 0.57\\\\\n",
      "& $\\infty$ & 1000 & 10.62 & 13.21 & 5.88 & 1.04 & 1.30 & 0.58\\\\\n",
      "\\hdashline \n",
      "\n",
      "gpt2\n",
      "&  &  & 12.51 & 14.16 & 6.59 & 1.18 & 1.34 & 0.62\\\\\n",
      "& 2500 & 500 & 11.88 & 14.04 & 5.16 & 1.12 & 1.33 & 0.49\\\\\n",
      "& 2500 & 1000 & 11.28 & 13.94 & 4.08 & 1.07 & 1.32 & 0.39\\\\\n",
      "& $\\infty$ & 500 & 12.41 & 14.14 & 6.52 & 1.17 & 1.34 & 0.62\\\\\n",
      "& $\\infty$ & 1000 & 12.37 & 14.13 & 6.45 & 1.17 & 1.34 & 0.61\\\\\n",
      "\\hdashline \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def renyi_entropy(P, alpha):\n",
    "    scale = 1 / (1 - alpha)\n",
    "\n",
    "    return scale * np.log2(np.sum([\n",
    "        prob**alpha\n",
    "        for prob in P\n",
    "    ]))\n",
    "\n",
    "def renyi_eff(P, alpha):\n",
    "    return renyi_entropy(P, alpha)/np.log(len(P))\n",
    "\n",
    "def shannon_entropy(P):\n",
    "    P = np.array(P)\n",
    "    return -np.sum(P * np.log2(P))\n",
    "\n",
    "def shannon_eff(P):\n",
    "    return shannon_entropy(P)/np.log(len(P))\n",
    "\n",
    "def table_line(P, extra=[]):\n",
    "    out = extra\n",
    "    out.append(f\"{shannon_entropy(P):.2f}\")\n",
    "    out.append(f\"{renyi_entropy(P, 0.5):.2f}\")\n",
    "    out.append(f\"{renyi_entropy(P, 3):.2f}\")\n",
    "    out.append(f\"{shannon_eff(P):.2f}\")\n",
    "    out.append(f\"{renyi_eff(P, 0.5):.2f}\")\n",
    "    out.append(f\"{renyi_eff(P, 3):.2f}\")\n",
    "    return \"& \" + \" & \".join(out) + r\"\\\\\"\n",
    "\n",
    "def freqs_to_p(freqs):\n",
    "    total = sum(freqs.values())\n",
    "    return [v/total for k, v in freqs.most_common()]\n",
    "\n",
    "def drop_bpe(freqs, unmerges, N, k):\n",
    "    freqs = copy.deepcopy(freqs)\n",
    "    # get top N\n",
    "    words_top_N = [k for k, v in freqs.most_common() if len(k) > 1][:N]\n",
    "\n",
    "    # sample k words\n",
    "    dead_tokens = random.sample(words_top_N, k=k)\n",
    "    for token in dead_tokens:\n",
    "        # add the old frequency to the individual characters\n",
    "        if token in unmerges:\n",
    "            # remove existing token\n",
    "            token_freq = freqs.pop(token)\n",
    "            for c in unmerges[token]:\n",
    "                freqs[c] += token_freq\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    return freqs_to_p(freqs)\n",
    "\n",
    "\n",
    "# NOTE for future self: the fact that the efficiency goes down is likely\n",
    "# caused by the unmerges vocabulary being made of observed vocabulary and not\n",
    "# the real unmerges (?)\n",
    "\n",
    "random.seed(0)\n",
    "for tokenizer, freqs in dists.items():\n",
    "    unmerges = vocab_to_unmerges(freqs)\n",
    "    print(tokenizer)\n",
    "    print(table_line(freqs_to_p(freqs), extra=[\"\", \"\"]))\n",
    "    print(table_line(drop_bpe(freqs, unmerges, 2_500, 500), extra=[\"2500\", \"500\"]))\n",
    "    print(table_line(drop_bpe(freqs, unmerges, 2_500, 1000), extra=[\"2500\", \"1000\"]))\n",
    "    print(table_line(drop_bpe(freqs, unmerges, None, 500), extra=[r\"$\\infty$\", \"500\"]))\n",
    "    print(table_line(drop_bpe(freqs, unmerges, None, 1000), extra=[r\"$\\infty$\", \"1000\"]))\n",
    "    print(r\"\\hdashline\", \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
