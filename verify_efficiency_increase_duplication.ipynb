{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/vzouhar/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "data = load_dataset(\"wikitext\", 'wikitext-103-v1', split=\"train\")\n",
    "data_1m = \"\"\n",
    "with open(\"data/wikitext_1m.txt\", \"w\") as f:\n",
    "    for line in data[\"text\"][:1_000_000]:\n",
    "        line = line.replace(\"<unk>\", \"\").strip()\n",
    "        if line:\n",
    "            f.write(line+\"\\n\")\n",
    "            data_1m += line+\"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 training\n",
      "\n",
      "\n",
      "\n",
      "8000 encoding\n",
      "28000 training\n",
      "\n",
      "\n",
      "\n",
      "28000 encoding\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import (\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "import collections\n",
    "dists = {}\n",
    "\n",
    "def get_bpe_unigram(vocab_size):\n",
    "    print(vocab_size, \"training\")\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size)\n",
    "    tokenizer.train([\"data/wikitext_1m.txt\"], trainer=trainer)\n",
    "    \n",
    "    print(vocab_size, \"encoding\")\n",
    "    encoding = tokenizer.encode(data_1m)\n",
    "    return collections.Counter(encoding.tokens)\n",
    "\n",
    "\n",
    "dists[\"bpe_8k\"] = get_bpe_unigram(8_000)\n",
    "dists[\"bpe_28k\"] = get_bpe_unigram(28_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (64274162 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "gpt2tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "# TODO: replace this with .tokenize call which might be slower but will make the types the same as for the bpe\n",
    "dists[\"gpt2\"] = collections.Counter(gpt2tokenizer.tokenize(data_1m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bpe_8k\n",
      "&  &  & 10.16 & 11.82 & 6.17 & 1.13 & 1.32 & 0.69\\\\\n",
      "& 100 & 3 & 10.86 & 12.01 & 7.75 & 1.21 & 1.33 & 0.86\\\\\n",
      "& 100 & 5 & 11.18 & 12.14 & 8.48 & 1.24 & 1.34 & 0.94\\\\\n",
      "& 500 & 3 & 11.16 & 12.28 & 7.75 & 1.23 & 1.35 & 0.85\\\\\n",
      "& 500 & 5 & 11.62 & 12.56 & 8.49 & 1.26 & 1.36 & 0.92\\\\\n",
      "\\hdashline \n",
      "\n",
      "bpe_28k\n",
      "&  &  & 10.65 & 13.29 & 5.83 & 1.04 & 1.30 & 0.57\\\\\n",
      "& 100 & 3 & 11.37 & 13.40 & 7.42 & 1.11 & 1.31 & 0.72\\\\\n",
      "& 100 & 5 & 11.71 & 13.48 & 8.15 & 1.14 & 1.32 & 0.80\\\\\n",
      "& 500 & 3 & 11.59 & 13.54 & 7.42 & 1.13 & 1.32 & 0.72\\\\\n",
      "& 500 & 5 & 12.02 & 13.71 & 8.16 & 1.17 & 1.33 & 0.79\\\\\n",
      "\\hdashline \n",
      "\n",
      "gpt2\n",
      "&  &  & 10.76 & 13.66 & 5.93 & 1.00 & 1.27 & 0.55\\\\\n",
      "& 100 & 3 & 11.49 & 13.77 & 7.52 & 1.07 & 1.28 & 0.70\\\\\n",
      "& 100 & 5 & 11.83 & 13.83 & 8.26 & 1.10 & 1.29 & 0.77\\\\\n",
      "& 500 & 3 & 11.70 & 13.89 & 7.52 & 1.09 & 1.29 & 0.70\\\\\n",
      "& 500 & 5 & 12.13 & 14.04 & 8.26 & 1.12 & 1.30 & 0.77\\\\\n",
      "\\hdashline \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def renyi_entropy(P, alpha):\n",
    "    scale = 1 / (1 - alpha)\n",
    "\n",
    "    return scale * np.log2(np.sum([\n",
    "        prob**alpha\n",
    "        for prob in P\n",
    "    ]))\n",
    "\n",
    "def renyi_eff(P, alpha):\n",
    "    return renyi_entropy(P, alpha)/np.log(len(P))\n",
    "\n",
    "def shannon_entropy(P):\n",
    "    P = np.array(P)\n",
    "    return -np.sum(P * np.log2(P))\n",
    "\n",
    "def shannon_eff(P):\n",
    "    return shannon_entropy(P)/np.log(len(P))\n",
    "\n",
    "def duplicate_bpe(P, N, k):\n",
    "    return P[N:]+[x/k for x in P[:N]] * k\n",
    "\n",
    "def table_line(P, extra=[]):\n",
    "    out = extra\n",
    "    out.append(f\"{shannon_entropy(P):.2f}\")\n",
    "    out.append(f\"{renyi_entropy(P, 0.5):.2f}\")\n",
    "    out.append(f\"{renyi_entropy(P, 3):.2f}\")\n",
    "    out.append(f\"{shannon_eff(P):.2f}\")\n",
    "    out.append(f\"{renyi_eff(P, 0.5):.2f}\")\n",
    "    out.append(f\"{renyi_eff(P, 3):.2f}\")\n",
    "    return \"& \" + \" & \".join(out) + r\"\\\\\"\n",
    "\n",
    "for tokenizer, freq in dists.items():\n",
    "    total = sum(freq.values())\n",
    "    P = [v/total for k, v in freq.most_common()]\n",
    "    print(tokenizer)\n",
    "    print(table_line(P, extra=[\"\", \"\"]))\n",
    "    print(table_line(duplicate_bpe(P, 100, 3), extra=[\"100\", \"3\"]))\n",
    "    print(table_line(duplicate_bpe(P, 100, 5), extra=[\"100\", \"5\"]))\n",
    "    print(table_line(duplicate_bpe(P, 500, 3), extra=[\"500\", \"3\"]))\n",
    "    print(table_line(duplicate_bpe(P, 500, 5), extra=[\"500\", \"5\"]))\n",
    "    print(r\"\\hdashline\", \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
